{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1VQQQrdjM_VRdGcYeTuvh50obj2-4X8SC","authorship_tag":"ABX9TyMEAEUKh7zva1QnPXuirl0J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Analisis Sentimen Ulasan Aplikasi Shopee Menggunakan Metode Klasifikasi Algoritma Naive Bayes"],"metadata":{"id":"Xl2FENV2XEVT"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/MyDrive/Kuliah/Semester 4/PENDAT /Ipyb/Analisis sentimen Lewat Ulasan di apk Shoope/Shopee_Sampled_Reviews.csv')\n"],"metadata":{"id":"7JrKRMzcWenz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# scraping data"],"metadata":{"id":"puwpaL6eW_SZ"}},{"cell_type":"code","source":["#Referensi: https://www.linkedin.com/pulse/how-scrape-google-play-reviews-4-simple-steps-using-python-kundi/\n","#download library google-play-scraper\n","!pip install google-play-scraper"],"metadata":{"id":"2U8B1asvXGxC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Impor paket yang diperlukan"],"metadata":{"id":"_EsElDLdXX5Z"}},{"cell_type":"code","source":["from google_play_scraper import app\n","\n","import pandas as pd\n","\n","import numpy as np"],"metadata":{"id":"ePV0ud1hXY5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#scrape jumlah ulasan yang diinginkan\n","from google_play_scraper import Sort, reviews\n","\n","result, continuation_token = reviews(\n","    'com.shopee.id',\n","    lang='id',  #disini kita mau men scrape data ulasan aplikasi shopee yang berada di google play store\n","    country='id', #kita setting bahasa nya menjadi bahasa indonesia\n","    sort=Sort.MOST_RELEVANT, # # kemudian kita gunakan most_relevan untuk mendapatkan ulasan yang paling relevant\n","    count=5000, # disini jumlah ulasan yang mau kita ambil ada seribu\n","    filter_score_with=None # # kemudian di filter_score kita gunakan None untuk mengambil semua score atau ratting bintang 1 sampai 5\n",")"],"metadata":{"id":"e4cW7bGvXcbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_busu = pd.DataFrame(np.array(result),columns=['review'])\n","\n","df_busu = df_busu.join(pd.DataFrame(df_busu.pop('review').tolist()))\n","\n","df_busu.head()"],"metadata":{"id":"meHOa3lYXf-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(df_busu.index) #kemudian hitung kembali berapa jumlah data yg didapatkan"],"metadata":{"id":"jN4_T-n3Xv5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_busu[['userName', 'score','at', 'content']].head()  #dari scrapping tsb didapatkan banyak sekali kolom, kemudian kolom\" tsb kita filter\n","                                                        #sehingga didapatkan kolom username, score, at dan content"],"metadata":{"id":"agTgALSJXxfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run This Code to Sort the Data By Date\n","\n","new_df = df_busu[['userName', 'score','at', 'content']]\n","sorted_df = new_df.sort_values(by='at', ascending=False) #Sort by Newst, change to True if you want to sort by Oldest.\n","sorted_df.head()"],"metadata":{"id":"kIxQsNYDXzfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df = sorted_df[['userName', 'score','at', 'content']] #kemudian kita simpan ke variabel my_df"],"metadata":{"id":"vq0F4-d8X0RK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df=my_df[['content', 'score']]#karena kita hanya membutuhkan kolom content dan score maka kita lakukan filter kolom lgi hingga menyisakan kolom content dan score.\n"],"metadata":{"id":"_i2M-rx6X2Ya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df.head()"],"metadata":{"id":"-sIZ3zslX4XK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PELABELAN"],"metadata":{"id":"-fY-p4kdX72b"}},{"cell_type":"code","source":["def pelabelan(score):\n","  if score < 3:\n","    return 'Negatif'\n","  elif score == 4 :\n","    return 'Positif'\n","  elif score == 5 :\n","    return 'Positif'\n","my_df['Label'] = my_df ['score'].apply(pelabelan)\n","my_df.head(10)"],"metadata":{"id":"NMZYrxbyX7Yy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df.to_csv(\"scrapped_data.csv\", index = False)  #kemudian save menjadi file csv"],"metadata":{"id":"ch8AXU67YBWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# pembersihan data\n","# data cleaning"],"metadata":{"id":"wPFvlvFmYFqr"}},{"cell_type":"code","source":["import pandas as pd\n","pd.set_option('display.max_columns', None)\n","my_df = pd.read_csv('/content/scrapped_data.csv')\n","my_df.head(10)"],"metadata":{"id":"HHzEb2nhYIL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# info() digunakan untuk menampilkan informasi detail tentang dataframe,\n","#seperti jumlah baris data, nama-nama kolom berserta jumlah data dan tipe datanya, dan sebagainya.\n","my_df.info()"],"metadata":{"id":"O6IHb1BPYJ7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tampilkan setiap baris yang memiliki nilai null (NaN) pada kolom apapun\n","#Gunakan fitur isna() yang disediakan library pandas\n","my_df.isna()"],"metadata":{"id":"Xxfmb7FRYLcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df.isna().any()"],"metadata":{"id":"hH3scu_6YNqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df.describe()"],"metadata":{"id":"k2ip0WLPYPLD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#mencari jumlah baris data yang bernilai null\n","#terdapat kolom label memiliki nilai kosong\n","my_df.isnull().sum()"],"metadata":{"id":"QHPmODp6YQ_K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Handling Missing value-Ignore tuple\n"],"metadata":{"id":"HMRY5Cb0YWkS"}},{"cell_type":"code","source":["my_df.dropna(subset=['Label'],inplace = True)"],"metadata":{"id":"lOMhzJMgYR7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df.isnull().sum()"],"metadata":{"id":"r8K8mWD9YZla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df.head(10)"],"metadata":{"id":"tBpIw0TkYcOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df.to_csv(\"shopeepreprocessing.csv\", index = False)  #simpan hasil file data cleaning dengan nama shopeepreprocessing.csv"],"metadata":{"id":"QLBizMy-YfBC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text PreProcessing"],"metadata":{"id":"ZYzWzP9qYiV6"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('/content/shopeepreprocessing.csv')\n","df.head(10)"],"metadata":{"id":"4OFkg-m-YlLi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Case Folding\n","Proses case folding adalah proses mengubah seluruh huruf menjadi huruf kecil. Pada proses ini karakter-karakter 'A'-'Z' yang terdapat pada data diubah kedalam karakter 'a'-'z'."],"metadata":{"id":"S0cUqgpfYupq"}},{"cell_type":"code","source":["import re\n","def  clean_text(df, text_field, new_text_field_name):\n","    my_df[new_text_field_name] = my_df[text_field].str.lower()\n","    my_df[new_text_field_name] = my_df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n","    # remove numbers\n","    my_df[new_text_field_name] = my_df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n","    return my_df"],"metadata":{"id":"HAlEntJXYoUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df['text_clean'] = my_df['content'].str.lower()\n","my_df['text_clean']\n","data_clean = clean_text(my_df, 'content', 'text_clean')\n","data_clean.head(10)\n"],"metadata":{"id":"a0KvQ7opYy3q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stopword Removal\n","Stopword adalah kata umum yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Contoh stopword dalam bahasa Indonesia adalah “yang”, “dan”, “di”, “dari”, dll. Makna di balik penggunaan stopword yaitu dengan menghapus kata-kata yang memiliki informasi rendah dari sebuah teks, kita dapat fokus pada kata-kata penting sebagai gantinya."],"metadata":{"id":"IyF04URpY19q"}},{"cell_type":"code","source":["import nltk.corpus\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop = stopwords.words('indonesian')\n","data_clean['text_StopWord'] = data_clean['text_clean'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n","data_clean.head(10)\n"],"metadata":{"id":"7c-K6HX1Y1GD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtDYoou7BPoE"},"source":["# Tokenizing\n","Tokenizing adalah proses pemisahan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian di analisa. Kata, angka, simbol, tanda baca dan entitas penting lainnya dapat dianggap sebagai token. Didalam NLP, token diartikan sebagai “kata” meskipun tokenize juga dapat dilakukan pada paragraf maupun kalimat"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","data_clean['text_tokens'] = data_clean['text_StopWord'].apply(lambda x: word_tokenize(x))\n","data_clean.head()"],"metadata":{"id":"be1BZTnFY4u6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stemming\n","Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya. Untuk melakukan stemming bahasa Indonesia kita dapat menggunakan library Python Sastrawi yang sudah kita siapkan di awal. Library Sastrawi menerapkan Algoritma Nazief dan Adriani dalam melakukan stemming bahasa Indonesia."],"metadata":{"id":"5TmaEABVZFCa"}},{"cell_type":"code","source":["!pip install Sastrawi"],"metadata":{"id":"9ts4fFp8ZGYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()"],"metadata":{"id":"7ncpi4suZIkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------STEMMING -----------------\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","#import swifter\n","\n","\n","# create stemmer\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","\n","# stemmed\n","def stemmed_wrapper(term):\n","    return stemmer.stem(term)\n","\n","term_dict = {}\n","hitung=0\n","\n","for document in data_clean['text_tokens']:\n","    for term in document:\n","        if term not in term_dict:\n","            term_dict[term] = ' '\n","\n","print(len(term_dict))\n","print(\"------------------------\")\n","for term in term_dict:\n","    term_dict[term] = stemmed_wrapper(term)\n","    hitung+=1\n","    print(hitung,\":\",term,\":\" ,term_dict[term])\n","\n","print(term_dict)\n","print(\"------------------------\")\n","\n","# apply stemmed term to dataframe\n","def get_stemmed_term(document):\n","    return [term_dict[term] for term in document]\n","\n","\n","#script ini bisa dipisah dari eksekusinya setelah pembacaaan term selesai\n","data_clean['text_steamindo'] = data_clean['text_tokens'].apply(lambda x:' '.join(get_stemmed_term(x)))\n","data_clean.head(20)"],"metadata":{"id":"p02zCWo_Zceq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_clean.to_csv('hasil_TextPreProcessing_shopee.csv', index= False) #kemudian simpan hasil text preprocessing ke file csv"],"metadata":{"id":"cWDR3o8AZhfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["spliting data\n","\n","memecah data test 20% dari keseluruhan data"],"metadata":{"id":"9ViIvP7YZm_j"}},{"cell_type":"code","source":["#membagi data menjadi data training dan testing dengan test_size = 0.20 dan random state nya 0\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data_clean['content'], data_clean['Label'],\n","                                                    test_size = 0.20,\n","                                                    random_state = 0)"],"metadata":{"id":"LL31AfdpZkvi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["pembobotan tf-idf"],"metadata":{"id":"wH---c0jZraK"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n","tfidf_test = tfidf_vectorizer.transform(X_test)\n"],"metadata":{"id":"BwLBy5tmZq-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"metadata":{"id":"KVePCyL2ZuNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","vectorizer.fit(X_train)"],"metadata":{"id":"l7G27NXlZwKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = vectorizer.transform(X_train)\n","X_test = vectorizer.transform(X_test)"],"metadata":{"id":"y4QnNYL2Zx4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","\n","nb = MultinomialNB()\n","nb.fit(tfidf_train, y_train)\n"],"metadata":{"id":"Nxo8MHYCZzL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.toarray()"],"metadata":{"id":"K5ymAL3mZ12i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = nb.predict(tfidf_test)\n"],"metadata":{"id":"lRLeia5RZ3Z6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","accuracy = accuracy_score(y_test, y_pred)\n"],"metadata":{"id":"txwfiAawZ40q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","predicted = clf.predict(X_test)\n","\n","print(\"MultinomialNB Accuracy:\", accuracy_score(y_test,predicted))\n","print(\"MultinomialNB Precision:\", precision_score(y_test,predicted, average=\"binary\", pos_label=\"Negatif\"))\n","print(\"MultinomialNB Recall:\", recall_score(y_test,predicted, average=\"binary\", pos_label=\"Negatif\"))\n","print(\"MultinomialNB f1_score:\", f1_score(y_test,predicted, average=\"binary\", pos_label=\"Negatif\"))\n","\n","print(f'confusion_matrix:\\n {confusion_matrix(y_test, predicted)}')\n","print('====================================================\\n')\n","print(classification_report(y_test, predicted, zero_division=0))\n","\n","# Load dataset\n","data_clean = pd.read_csv('hasil_TextPreProcessing_shopee.csv')\n","\n","#\n"],"metadata":{"id":"ZffEDNw9Z7vK"},"execution_count":null,"outputs":[]}]}